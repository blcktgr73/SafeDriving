{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 안전 운전 데이터 탐색적 데이터 분석 (EDA)\n",
    "\n",
    "이 노트북은 SafeDriving 프로젝트의 데이터를 탐색하고 분석하는 과정을 다룹니다.\n",
    "\n",
    "## 🎯 분석 목표\n",
    "1. **데이터 이해**: 수집된 데이터의 구조와 특성 파악\n",
    "2. **패턴 발견**: 안전 운전과 관련된 패턴 및 인사이트 도출\n",
    "3. **피처 중요도**: 안전 운전 예측에 중요한 변수들 식별\n",
    "4. **데이터 품질**: 결측값, 이상값, 분포 특성 분석\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 라이브러리 및 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "# 통계 및 분석\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 시스템 경로 설정\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "# 프로젝트 모듈\n",
    "from data.data_loader import SafeDrivingDataLoader\n",
    "from data.preprocessor import SafeDrivingPreprocessor\n",
    "\n",
    "# 설정\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 시각화 설정\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# 한글 폰트 설정 (한국어 사용시)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"📊 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 데이터 로드 및 기본 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 초기화\n",
    "loader = SafeDrivingDataLoader()\n",
    "\n",
    "# 모든 데이터셋 로드\n",
    "print(\"🔄 데이터 로드 중...\")\n",
    "all_data = loader.load_all_data()\n",
    "\n",
    "print(f\"✅ 총 {len(all_data)}개의 데이터셋 로드 완료!\")\n",
    "for name, df in all_data.items():\n",
    "    print(f\"  📋 {name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 데이터셋 세부 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터셋 상세 정보 출력\n",
    "for data_name, df in all_data.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"📊 {data_name.upper()} 데이터셋 분석\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    info = loader.get_data_info(df, data_name)\n",
    "    \n",
    "    print(f\"🔍 기본 정보:\")\n",
    "    print(f\"  - Shape: {info['shape']}\")\n",
    "    print(f\"  - 수치형 컬럼: {len(info['numeric_columns'])}개\")\n",
    "    print(f\"  - 범주형 컬럼: {len(info['categorical_columns'])}개\")\n",
    "    print(f\"  - 메모리 사용량: {info['memory_usage'] / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # 결측값 정보\n",
    "    missing_count = sum(info['missing_values'].values())\n",
    "    if missing_count > 0:\n",
    "        print(f\"⚠️ 결측값: {missing_count}개\")\n",
    "        missing_cols = {k: v for k, v in info['missing_values'].items() if v > 0}\n",
    "        for col, count in missing_cols.items():\n",
    "            print(f\"    - {col}: {count}개 ({count/info['shape'][0]*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"✅ 결측값: 없음\")\n",
    "    \n",
    "    # 타겟 분포 (있는 경우)\n",
    "    if 'target_distribution' in info:\n",
    "        print(f\"🎯 타겟 분포: {info['target_distribution']}\")\n",
    "        target_ratio = info['target_distribution'].get(1, 0) / info['shape'][0]\n",
    "        print(f\"  - 긍정 클래스 비율: {target_ratio:.2%}\")\n",
    "    \n",
    "    # 컬럼 미리보기\n",
    "    print(f\"\\n📋 컬럼 목록 (처음 10개):\")\n",
    "    for i, col in enumerate(info['columns'][:10]):\n",
    "        dtype = info['dtypes'][col]\n",
    "        print(f\"  {i+1:2d}. {col:<20} ({dtype})\")\n",
    "    \n",
    "    if len(info['columns']) > 10:\n",
    "        print(f\"     ... 외 {len(info['columns'])-10}개 컬럼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Kaggle 데이터 심화 분석\n",
    "\n",
    "Kaggle Safe Driving 데이터의 특성을 자세히 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle 훈련 데이터 선택\n",
    "if 'kaggle_train' in all_data:\n",
    "    kaggle_data = all_data['kaggle_train']\n",
    "    print(f\"📊 Kaggle 훈련 데이터 분석: {kaggle_data.shape}\")\n",
    "    print(f\"📋 처음 5행 미리보기:\")\n",
    "    display(kaggle_data.head())\n",
    "else:\n",
    "    print(\"⚠️ Kaggle 훈련 데이터를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 타겟 변수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kaggle_train' in all_data and 'target' in kaggle_data.columns:\n",
    "    # 타겟 분포 시각화\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 막대 그래프\n",
    "    target_counts = kaggle_data['target'].value_counts()\n",
    "    axes[0].bar(target_counts.index, target_counts.values, color=['skyblue', 'salmon'])\n",
    "    axes[0].set_title('타겟 변수 분포', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('타겟 값')\n",
    "    axes[0].set_ylabel('빈도')\n",
    "    \n",
    "    # 비율 표시\n",
    "    for i, v in enumerate(target_counts.values):\n",
    "        axes[0].text(i, v + target_counts.max() * 0.01, \n",
    "                    f'{v:,}\\n({v/len(kaggle_data)*100:.1f}%)', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 파이 차트\n",
    "    labels = ['안전 (0)', '위험 (1)']\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    axes[1].pie(target_counts.values, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "               startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "    axes[1].set_title('타겟 변수 비율', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 클래스 불균형 분석\n",
    "    imbalance_ratio = target_counts[0] / target_counts[1]\n",
    "    print(f\"\\n📈 클래스 불균형 분석:\")\n",
    "    print(f\"  - 안전(0): {target_counts[0]:,}개 ({target_counts[0]/len(kaggle_data)*100:.1f}%)\")\n",
    "    print(f\"  - 위험(1): {target_counts[1]:,}개 ({target_counts[1]/len(kaggle_data)*100:.1f}%)\")\n",
    "    print(f\"  - 불균형 비율: {imbalance_ratio:.1f}:1\")\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"⚠️ 심한 클래스 불균형이 감지되었습니다. SMOTE나 클래스 가중치 조정을 고려해야 합니다.\")\n",
    "    elif imbalance_ratio > 5:\n",
    "        print(f\"⚠️ 중간 정도의 클래스 불균형이 있습니다. 균형 조정 기법을 고려해볼 수 있습니다.\")\n",
    "    else:\n",
    "        print(f\"✅ 클래스 균형이 비교적 양호합니다.\")\n",
    "else:\n",
    "    print(\"⚠️ 타겟 변수를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔢 수치형 변수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kaggle_train' in all_data:\n",
    "    # 수치형 컬럼 선별\n",
    "    numeric_cols = kaggle_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'id' in numeric_cols:\n",
    "        numeric_cols.remove('id')\n",
    "    if 'target' in numeric_cols:\n",
    "        numeric_cols.remove('target')\n",
    "    \n",
    "    print(f\"📊 수치형 변수 {len(numeric_cols)}개 분석\")\n",
    "    \n",
    "    # 기본 통계량\n",
    "    print(f\"\\n📈 기본 통계량:\")\n",
    "    display(kaggle_data[numeric_cols].describe().round(3))\n",
    "    \n",
    "    # 상관관계 분석 (샘플링으로 속도 향상)\n",
    "    sample_size = min(len(kaggle_data), 5000)\n",
    "    sample_data = kaggle_data.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # 타겟과의 상관관계가 높은 변수들 찾기\n",
    "    if 'target' in kaggle_data.columns:\n",
    "        correlations = sample_data[numeric_cols + ['target']].corr()['target'].drop('target')\n",
    "        correlations_abs = correlations.abs().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\n🎯 타겟과 상관관계가 높은 변수 TOP 10:\")\n",
    "        for i, (var, corr) in enumerate(correlations_abs.head(10).items()):\n",
    "            print(f\"  {i+1:2d}. {var:<20} : {correlations[var]:>7.4f} (|{corr:.4f}|)\")\n",
    "        \n",
    "        # 상관관계 히트맵 (상위 변수들만)\n",
    "        top_vars = correlations_abs.head(15).index.tolist() + ['target']\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        corr_matrix = sample_data[top_vars].corr()\n",
    "        \n",
    "        # 마스크 생성 (상삼각형 숨김)\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        \n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
    "                   square=True, fmt='.3f', cbar_kws={\"shrink\": .8})\n",
    "        plt.title('주요 변수들의 상관관계 히트맵', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Kaggle 데이터를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 주요 변수 분포 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kaggle_train' in all_data and len(numeric_cols) > 0:\n",
    "    # 상위 중요 변수들의 분포 시각화\n",
    "    if 'target' in kaggle_data.columns:\n",
    "        top_vars = correlations_abs.head(8).index.tolist()\n",
    "    else:\n",
    "        top_vars = numeric_cols[:8]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, var in enumerate(top_vars):\n",
    "        # 히스토그램\n",
    "        axes[i].hist(kaggle_data[var].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_title(f'{var}', fontweight='bold')\n",
    "        axes[i].set_ylabel('빈도')\n",
    "        \n",
    "        # 기본 통계량 표시\n",
    "        mean_val = kaggle_data[var].mean()\n",
    "        median_val = kaggle_data[var].median()\n",
    "        axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'평균: {mean_val:.3f}')\n",
    "        axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.8, label=f'중앙값: {median_val:.3f}')\n",
    "        axes[i].legend(fontsize=8)\n",
    "        \n",
    "        # 이상값 표시\n",
    "        Q1 = kaggle_data[var].quantile(0.25)\n",
    "        Q3 = kaggle_data[var].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_threshold = Q3 + 1.5 * IQR\n",
    "        outliers = kaggle_data[var] > outlier_threshold\n",
    "        \n",
    "        if outliers.sum() > 0:\n",
    "            axes[i].axvline(outlier_threshold, color='orange', linestyle=':', alpha=0.8)\n",
    "            axes[i].text(0.95, 0.95, f'이상값: {outliers.sum()}개', \n",
    "                        transform=axes[i].transAxes, ha='right', va='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle('주요 변수들의 분포', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 분포 특성 분석\n",
    "    print(f\"\\n📊 분포 특성 분석:\")\n",
    "    for var in top_vars:\n",
    "        data = kaggle_data[var].dropna()\n",
    "        \n",
    "        # 왜도와 첨도\n",
    "        skewness = stats.skew(data)\n",
    "        kurtosis = stats.kurtosis(data)\n",
    "        \n",
    "        # 정규성 검정 (샘플링)\n",
    "        if len(data) > 5000:\n",
    "            test_sample = data.sample(5000, random_state=42)\n",
    "        else:\n",
    "            test_sample = data\n",
    "        \n",
    "        _, p_value = stats.normaltest(test_sample)\n",
    "        \n",
    "        print(f\"  📈 {var:<20}: 왜도={skewness:>6.2f}, 첨도={kurtosis:>6.2f}, p-value={p_value:.2e}\")\n",
    "        \n",
    "        if abs(skewness) > 2:\n",
    "            print(f\"      ⚠️ 심한 비대칭 분포 (로그 변환 고려)\")\n",
    "        elif abs(skewness) > 1:\n",
    "            print(f\"      ⚠️ 중간 정도 비대칭 분포\")\n",
    "        \n",
    "        if p_value < 0.001:\n",
    "            print(f\"      ⚠️ 비정규분포 (변환 필요)\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ 분석할 수치형 변수가 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚗 센서 데이터 분석\n",
    "\n",
    "GPS, 가속도계, 자이로스코프 등 센서 데이터의 특성을 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 센서 데이터 선별\n",
    "sensor_datasets = {k: v for k, v in all_data.items() if 'sensor' in k}\n",
    "\n",
    "print(f\"🔍 센서 데이터셋 {len(sensor_datasets)}개 발견:\")\n",
    "for name, df in sensor_datasets.items():\n",
    "    print(f\"  📡 {name}: {df.shape}\")\n",
    "\n",
    "# 센서 데이터 통합 분석\n",
    "if sensor_datasets:\n",
    "    # 첫 번째 센서 데이터셋으로 상세 분석\n",
    "    first_sensor = list(sensor_datasets.keys())[0]\n",
    "    sensor_data = sensor_datasets[first_sensor]\n",
    "    \n",
    "    print(f\"\\n📊 {first_sensor} 데이터 상세 분석:\")\n",
    "    print(f\"📋 컬럼 정보:\")\n",
    "    display(sensor_data.info())\n",
    "    print(f\"\\n📋 처음 5행:\")\n",
    "    display(sensor_data.head())\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ 센서 데이터를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🗺️ GPS 데이터 분석 (있는 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPS 데이터 찾기\n",
    "gps_data = None\n",
    "for name, df in sensor_datasets.items():\n",
    "    if 'gps' in name and all(col in df.columns for col in ['latitude', 'longitude']):\n",
    "        gps_data = df\n",
    "        print(f\"🗺️ GPS 데이터 발견: {name}\")\n",
    "        break\n",
    "\n",
    "if gps_data is not None:\n",
    "    # GPS 데이터 기본 통계\n",
    "    print(f\"\\n📊 GPS 데이터 기본 통계:\")\n",
    "    gps_stats = gps_data[['latitude', 'longitude']].describe()\n",
    "    display(gps_stats)\n",
    "    \n",
    "    # 이동 범위 계산\n",
    "    lat_range = gps_data['latitude'].max() - gps_data['latitude'].min()\n",
    "    lon_range = gps_data['longitude'].max() - gps_data['longitude'].min()\n",
    "    \n",
    "    print(f\"\\n🗺️ 이동 범위:\")\n",
    "    print(f\"  - 위도 범위: {lat_range:.6f}° ({lat_range * 111:.1f} km)\")\n",
    "    print(f\"  - 경도 범위: {lon_range:.6f}° ({lon_range * 111:.1f} km)\")\n",
    "    \n",
    "    # GPS 궤적 시각화\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 전체 궤적\n",
    "    plt.scatter(gps_data['longitude'], gps_data['latitude'], \n",
    "               c=range(len(gps_data)), cmap='viridis', alpha=0.6, s=1)\n",
    "    plt.colorbar(label='시간 순서')\n",
    "    plt.xlabel('경도 (Longitude)')\n",
    "    plt.ylabel('위도 (Latitude)')\n",
    "    plt.title('GPS 이동 궤적', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 시작점과 끝점 표시\n",
    "    plt.scatter(gps_data['longitude'].iloc[0], gps_data['latitude'].iloc[0], \n",
    "               color='green', s=100, marker='o', label='시작점', zorder=5)\n",
    "    plt.scatter(gps_data['longitude'].iloc[-1], gps_data['latitude'].iloc[-1], \n",
    "               color='red', s=100, marker='s', label='끝점', zorder=5)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 속도 분석 (있는 경우)\n",
    "    if 'speed_kmh' in gps_data.columns:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # 시간별 속도 변화\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(gps_data.index, gps_data['speed_kmh'], linewidth=0.8, alpha=0.7)\n",
    "        plt.title('시간별 속도 변화', fontweight='bold')\n",
    "        plt.xlabel('시간 인덱스')\n",
    "        plt.ylabel('속도 (km/h)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 속도 분포\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(gps_data['speed_kmh'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(gps_data['speed_kmh'].mean(), color='red', linestyle='--', \n",
    "                   label=f'평균: {gps_data[\"speed_kmh\"].mean():.1f} km/h')\n",
    "        plt.title('속도 분포', fontweight='bold')\n",
    "        plt.xlabel('속도 (km/h)')\n",
    "        plt.ylabel('빈도')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 속도 통계\n",
    "        print(f\"\\n🚗 속도 통계:\")\n",
    "        print(f\"  - 평균 속도: {gps_data['speed_kmh'].mean():.1f} km/h\")\n",
    "        print(f\"  - 최대 속도: {gps_data['speed_kmh'].max():.1f} km/h\")\n",
    "        print(f\"  - 속도 표준편차: {gps_data['speed_kmh'].std():.1f} km/h\")\n",
    "        \n",
    "        # 위험 속도 구간 분석\n",
    "        high_speed = (gps_data['speed_kmh'] > 80).sum()\n",
    "        very_high_speed = (gps_data['speed_kmh'] > 120).sum()\n",
    "        \n",
    "        print(f\"  - 고속 구간 (>80km/h): {high_speed}회 ({high_speed/len(gps_data)*100:.1f}%)\")\n",
    "        print(f\"  - 초고속 구간 (>120km/h): {very_high_speed}회 ({very_high_speed/len(gps_data)*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ GPS 데이터를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📱 가속도계 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가속도계 데이터 찾기\n",
    "acc_data = None\n",
    "for name, df in sensor_datasets.items():\n",
    "    if 'accelerometer' in name or any('acc_' in col for col in df.columns):\n",
    "        acc_data = df\n",
    "        print(f\"📱 가속도계 데이터 발견: {name}\")\n",
    "        break\n",
    "\n",
    "if acc_data is not None:\n",
    "    # 가속도 컬럼 식별\n",
    "    acc_cols = [col for col in acc_data.columns if col.startswith('acc_')]\n",
    "    \n",
    "    if len(acc_cols) >= 3:\n",
    "        print(f\"\\n📊 가속도계 데이터 분석 ({len(acc_cols)}개 축):\")\n",
    "        \n",
    "        # 가속도 기본 통계\n",
    "        acc_stats = acc_data[acc_cols].describe()\n",
    "        display(acc_stats)\n",
    "        \n",
    "        # 3축 가속도 시각화\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 시계열 플롯\n",
    "        sample_size = min(1000, len(acc_data))\n",
    "        sample_indices = np.linspace(0, len(acc_data)-1, sample_size, dtype=int)\n",
    "        \n",
    "        axes[0, 0].plot(sample_indices, acc_data[acc_cols[0]].iloc[sample_indices], label=acc_cols[0], alpha=0.7)\n",
    "        axes[0, 0].plot(sample_indices, acc_data[acc_cols[1]].iloc[sample_indices], label=acc_cols[1], alpha=0.7)\n",
    "        axes[0, 0].plot(sample_indices, acc_data[acc_cols[2]].iloc[sample_indices], label=acc_cols[2], alpha=0.7)\n",
    "        axes[0, 0].set_title('3축 가속도 시계열', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('시간 인덱스')\n",
    "        axes[0, 0].set_ylabel('가속도 (m/s²)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 총 가속도 크기 계산\n",
    "        total_acc = np.sqrt(acc_data[acc_cols[0]]**2 + acc_data[acc_cols[1]]**2 + acc_data[acc_cols[2]]**2)\n",
    "        \n",
    "        # 총 가속도 분포\n",
    "        axes[0, 1].hist(total_acc.dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].axvline(total_acc.mean(), color='red', linestyle='--', \n",
    "                          label=f'평균: {total_acc.mean():.2f}')\n",
    "        axes[0, 1].set_title('총 가속도 크기 분포', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('총 가속도 (m/s²)')\n",
    "        axes[0, 1].set_ylabel('빈도')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 가속도 변동성 (Jerk) 분석\n",
    "        jerk = total_acc.diff().abs()\n",
    "        axes[1, 0].plot(sample_indices[1:], jerk.iloc[sample_indices[1:]], alpha=0.7)\n",
    "        axes[1, 0].set_title('가속도 변동성 (Jerk)', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('시간 인덱스')\n",
    "        axes[1, 0].set_ylabel('Jerk (m/s³)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 고가속도 이벤트 탐지\n",
    "        high_acc_threshold = total_acc.quantile(0.95)\n",
    "        high_acc_events = total_acc > high_acc_threshold\n",
    "        \n",
    "        axes[1, 1].scatter(range(len(total_acc)), total_acc, alpha=0.3, s=1)\n",
    "        axes[1, 1].scatter(range(len(total_acc))[high_acc_events], \n",
    "                          total_acc[high_acc_events], color='red', s=2, \n",
    "                          label=f'고가속도 이벤트 ({high_acc_events.sum()}개)')\n",
    "        axes[1, 1].axhline(high_acc_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[1, 1].set_title('고가속도 이벤트 탐지', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('시간 인덱스')\n",
    "        axes[1, 1].set_ylabel('총 가속도 (m/s²)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 가속도 통계 요약\n",
    "        print(f\"\\n📊 가속도 분석 결과:\")\n",
    "        print(f\"  - 평균 총 가속도: {total_acc.mean():.2f} ± {total_acc.std():.2f} m/s²\")\n",
    "        print(f\"  - 최대 총 가속도: {total_acc.max():.2f} m/s²\")\n",
    "        print(f\"  - 고가속도 이벤트 (상위 5%): {high_acc_events.sum()}개 ({high_acc_events.sum()/len(total_acc)*100:.1f}%)\")\n",
    "        print(f\"  - 평균 Jerk: {jerk.mean():.2f} ± {jerk.std():.2f} m/s³\")\n",
    "        \n",
    "        # 중력 제거 분석 (Z축이 중력 방향이라고 가정)\n",
    "        if 'acc_z' in acc_cols:\n",
    "            gravity_removed = np.sqrt(acc_data['acc_x']**2 + acc_data['acc_y']**2 + (acc_data['acc_z'] - 9.8)**2)\n",
    "            print(f\"  - 중력 제거 후 평균 가속도: {gravity_removed.mean():.2f} ± {gravity_removed.std():.2f} m/s²\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"⚠️ 3축 가속도 데이터가 완전하지 않습니다. 발견된 컬럼: {acc_cols}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ 가속도계 데이터를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 피처 엔지니어링 미리보기\n",
    "\n",
    "전처리 모듈을 사용해서 파생 변수들을 생성하고 그 효과를 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 모듈 초기화\n",
    "preprocessor = SafeDrivingPreprocessor()\n",
    "\n",
    "# 센서 데이터로 피처 엔지니어링 테스트\n",
    "if sensor_datasets:\n",
    "    # 첫 번째 센서 데이터 선택\n",
    "    test_data = list(sensor_datasets.values())[0].copy()\n",
    "    \n",
    "    print(f\"🔧 피처 엔지니어링 테스트\")\n",
    "    print(f\"원본 데이터: {test_data.shape}\")\n",
    "    \n",
    "    # 파생 변수 생성\n",
    "    enhanced_data = preprocessor.create_derived_features(test_data)\n",
    "    print(f\"파생 변수 추가 후: {enhanced_data.shape}\")\n",
    "    \n",
    "    # 새로 생성된 변수들\n",
    "    new_features = [col for col in enhanced_data.columns if col not in test_data.columns]\n",
    "    print(f\"\\n🆕 새로 생성된 피처 {len(new_features)}개:\")\n",
    "    \n",
    "    for i, feature in enumerate(new_features[:15]):  # 상위 15개만 표시\n",
    "        print(f\"  {i+1:2d}. {feature}\")\n",
    "    \n",
    "    if len(new_features) > 15:\n",
    "        print(f\"     ... 외 {len(new_features)-15}개\")\n",
    "    \n",
    "    # 중요한 안전 운전 피처들 시각화\n",
    "    safety_features = [col for col in new_features \n",
    "                      if any(keyword in col.lower() for keyword in \n",
    "                            ['harsh', 'speed_change', 'acceleration', 'turn'])]\n",
    "    \n",
    "    if len(safety_features) > 0:\n",
    "        n_features = min(4, len(safety_features))\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            feature = safety_features[i]\n",
    "            data = enhanced_data[feature].dropna()\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                axes[i].hist(data, bins=30, alpha=0.7, edgecolor='black')\n",
    "                axes[i].set_title(feature, fontweight='bold')\n",
    "                axes[i].set_ylabel('빈도')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # 기본 통계량 표시\n",
    "                mean_val = data.mean()\n",
    "                axes[i].axvline(mean_val, color='red', linestyle='--', \n",
    "                               label=f'평균: {mean_val:.3f}')\n",
    "                axes[i].legend()\n",
    "        \n",
    "        plt.suptitle('주요 안전 운전 파생 피처들', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 안전 운전 점수 피처 생성 테스트\n",
    "        scored_data = preprocessor.create_driving_score_features(enhanced_data)\n",
    "        \n",
    "        score_features = [col for col in scored_data.columns \n",
    "                         if 'score' in col.lower() or 'freq' in col.lower()]\n",
    "        \n",
    "        if score_features:\n",
    "            print(f\"\\n🏆 안전 운전 점수 피처들:\")\n",
    "            for feature in score_features:\n",
    "                data = scored_data[feature].dropna()\n",
    "                if len(data) > 0:\n",
    "                    print(f\"  - {feature:<25}: 평균 {data.mean():.3f} (범위: {data.min():.3f} ~ {data.max():.3f})\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ 피처 엔지니어링 테스트를 위한 센서 데이터가 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 데이터 품질 종합 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 데이터 품질 종합 평가 리포트\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_datasets = len(all_data)\n",
    "total_samples = sum(df.shape[0] for df in all_data.values())\n",
    "total_features = sum(df.shape[1] for df in all_data.values())\n",
    "\n",
    "print(f\"\\n📋 전체 데이터 요약:\")\n",
    "print(f\"  - 데이터셋 수: {total_datasets}개\")\n",
    "print(f\"  - 총 샘플 수: {total_samples:,}개\")\n",
    "print(f\"  - 총 피처 수: {total_features:,}개\")\n",
    "\n",
    "# 데이터셋별 품질 평가\n",
    "quality_scores = []\n",
    "\n",
    "for name, df in all_data.items():\n",
    "    # 품질 지표 계산\n",
    "    missing_ratio = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])\n",
    "    duplicate_ratio = df.duplicated().sum() / df.shape[0]\n",
    "    \n",
    "    # 수치형 변수의 분산\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        variance_score = df[numeric_cols].var().mean()\n",
    "    else:\n",
    "        variance_score = 0\n",
    "    \n",
    "    # 종합 품질 점수 (0-100)\n",
    "    quality_score = (\n",
    "        (1 - missing_ratio) * 40 +  # 결측값이 적을수록 좋음\n",
    "        (1 - duplicate_ratio) * 30 +  # 중복이 적을수록 좋음\n",
    "        min(variance_score / 10, 1) * 30  # 적당한 분산이 좋음\n",
    "    ) * 100\n",
    "    \n",
    "    quality_scores.append({\n",
    "        'dataset': name,\n",
    "        'samples': df.shape[0],\n",
    "        'features': df.shape[1],\n",
    "        'missing_ratio': missing_ratio,\n",
    "        'duplicate_ratio': duplicate_ratio,\n",
    "        'quality_score': quality_score\n",
    "    })\n",
    "\n",
    "# 품질 점수 데이터프레임\n",
    "quality_df = pd.DataFrame(quality_scores)\n",
    "quality_df = quality_df.sort_values('quality_score', ascending=False)\n",
    "\n",
    "print(f\"\\n🏆 데이터셋별 품질 점수 (100점 만점):\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in quality_df.iterrows():\n",
    "    score_emoji = \"🟢\" if row['quality_score'] >= 80 else \"🟡\" if row['quality_score'] >= 60 else \"🔴\"\n",
    "    print(f\"{score_emoji} {row['dataset']:<20}: {row['quality_score']:>5.1f}점 \"\n",
    "          f\"(결측률: {row['missing_ratio']*100:>4.1f}%, 중복률: {row['duplicate_ratio']*100:>4.1f}%)\")\n",
    "\n",
    "# 품질 개선 제안\n",
    "print(f\"\\n💡 품질 개선 제안:\")\n",
    "\n",
    "low_quality = quality_df[quality_df['quality_score'] < 70]\n",
    "if len(low_quality) > 0:\n",
    "    print(f\"  ⚠️ 품질이 낮은 데이터셋 {len(low_quality)}개:\")\n",
    "    for _, row in low_quality.iterrows():\n",
    "        print(f\"    - {row['dataset']}: \", end=\"\")\n",
    "        if row['missing_ratio'] > 0.1:\n",
    "            print(\"결측값 처리 필요 \", end=\"\")\n",
    "        if row['duplicate_ratio'] > 0.05:\n",
    "            print(\"중복값 제거 필요 \", end=\"\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"  ✅ 모든 데이터셋이 양호한 품질을 보입니다!\")\n",
    "\n",
    "# 전체 품질 점수\n",
    "overall_quality = quality_df['quality_score'].mean()\n",
    "overall_emoji = \"🟢\" if overall_quality >= 80 else \"🟡\" if overall_quality >= 60 else \"🔴\"\n",
    "\n",
    "print(f\"\\n{overall_emoji} 전체 데이터 품질 점수: {overall_quality:.1f}점\")\n",
    "\n",
    "if overall_quality >= 80:\n",
    "    print(f\"  ✅ 우수한 데이터 품질! 모델 학습에 적합합니다.\")\n",
    "elif overall_quality >= 60:\n",
    "    print(f\"  ⚠️ 보통 수준의 데이터 품질. 일부 전처리가 필요합니다.\")\n",
    "else:\n",
    "    print(f\"  🔴 데이터 품질 개선이 시급합니다. 상당한 전처리 작업이 필요합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 주요 발견사항 및 다음 단계\n",
    "\n",
    "### 📊 주요 발견사항\n",
    "\n",
    "이 EDA를 통해 발견된 주요 인사이트들을 정리합니다:\n",
    "\n",
    "1. **데이터 구조 분석**\n",
    "   - 전체 데이터셋 구성과 각각의 특성 파악 완료\n",
    "   - 결측값과 이상값 패턴 식별\n",
    "\n",
    "2. **안전 운전 패턴**\n",
    "   - 타겟 변수의 클래스 불균형 정도 확인\n",
    "   - 주요 위험 요인들과의 상관관계 분석\n",
    "\n",
    "3. **센서 데이터 특성**\n",
    "   - GPS 궤적과 속도 패턴 분석\n",
    "   - 가속도계 데이터의 노이즈와 신호 특성\n",
    "\n",
    "4. **피처 엔지니어링 가능성**\n",
    "   - 급정지/급가속 감지 가능성 확인\n",
    "   - 운전 패턴 기반 안전 점수 계산 가능성\n",
    "\n",
    "### 🔄 다음 단계 (Phase 3)\n",
    "\n",
    "1. **피처 엔지니어링 심화**\n",
    "   - 운전 패턴 기반 피처 개발\n",
    "   - 시간 윈도우 기반 집계 피처 생성\n",
    "\n",
    "2. **모델 개발 준비**\n",
    "   - 클래스 불균형 해결 전략 수립\n",
    "   - 피처 선택 및 차원 축소 검토\n",
    "\n",
    "3. **베이스라인 모델 구축**\n",
    "   - XGBoost/LightGBM 모델 개발\n",
    "   - PyTorch MLP 모델 구축\n",
    "\n",
    "---\n",
    "\n",
    "**다음 노트북**: `02_Feature_Engineering.ipynb` 에서 더 상세한 피처 엔지니어링을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 EDA 완료!\")\n",
    "print(\"다음 단계: 피처 엔지니어링 및 모델 개발\")\n",
    "\n",
    "# 분석 결과 요약 저장 (선택사항)\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "eda_summary = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'total_datasets': len(all_data),\n",
    "    'total_samples': sum(df.shape[0] for df in all_data.values()),\n",
    "    'total_features': sum(df.shape[1] for df in all_data.values()),\n",
    "    'data_quality': {\n",
    "        'overall_score': overall_quality,\n",
    "        'datasets': quality_scores\n",
    "    },\n",
    "    'next_steps': [\n",
    "        'Feature Engineering',\n",
    "        'Model Development',\n",
    "        'Hyperparameter Optimization'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 결과 저장\n",
    "with open('../results/eda_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eda_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"📄 EDA 요약 결과가 '../results/eda_summary.json'에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}