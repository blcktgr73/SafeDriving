{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š ì•ˆì „ ìš´ì „ ë°ì´í„° íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ SafeDriving í”„ë¡œì íŠ¸ì˜ ë°ì´í„°ë¥¼ íƒìƒ‰í•˜ê³  ë¶„ì„í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ ë¶„ì„ ëª©í‘œ\n",
    "1. **ë°ì´í„° ì´í•´**: ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŠ¹ì„± íŒŒì•…\n",
    "2. **íŒ¨í„´ ë°œê²¬**: ì•ˆì „ ìš´ì „ê³¼ ê´€ë ¨ëœ íŒ¨í„´ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ\n",
    "3. **í”¼ì²˜ ì¤‘ìš”ë„**: ì•ˆì „ ìš´ì „ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ë³€ìˆ˜ë“¤ ì‹ë³„\n",
    "4. **ë°ì´í„° í’ˆì§ˆ**: ê²°ì¸¡ê°’, ì´ìƒê°’, ë¶„í¬ íŠ¹ì„± ë¶„ì„\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ëª¨ë“ˆ import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "# í†µê³„ ë° ë¶„ì„\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ì‹œìŠ¤í…œ ê²½ë¡œ ì„¤ì •\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ\n",
    "from data.data_loader import SafeDrivingDataLoader\n",
    "from data.preprocessor import SafeDrivingPreprocessor\n",
    "\n",
    "# ì„¤ì •\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (í•œêµ­ì–´ ì‚¬ìš©ì‹œ)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"ğŸ“Š ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë” ì´ˆê¸°í™”\n",
    "loader = SafeDrivingDataLoader()\n",
    "\n",
    "# ëª¨ë“  ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "print(\"ğŸ”„ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "all_data = loader.load_all_data()\n",
    "\n",
    "print(f\"âœ… ì´ {len(all_data)}ê°œì˜ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
    "for name, df in all_data.items():\n",
    "    print(f\"  ğŸ“‹ {name}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š ë°ì´í„°ì…‹ ì„¸ë¶€ ì •ë³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° ë°ì´í„°ì…‹ ìƒì„¸ ì •ë³´ ì¶œë ¥\n",
    "for data_name, df in all_data.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ğŸ“Š {data_name.upper()} ë°ì´í„°ì…‹ ë¶„ì„\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    info = loader.get_data_info(df, data_name)\n",
    "    \n",
    "    print(f\"ğŸ” ê¸°ë³¸ ì •ë³´:\")\n",
    "    print(f\"  - Shape: {info['shape']}\")\n",
    "    print(f\"  - ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(info['numeric_columns'])}ê°œ\")\n",
    "    print(f\"  - ë²”ì£¼í˜• ì»¬ëŸ¼: {len(info['categorical_columns'])}ê°œ\")\n",
    "    print(f\"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {info['memory_usage'] / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # ê²°ì¸¡ê°’ ì •ë³´\n",
    "    missing_count = sum(info['missing_values'].values())\n",
    "    if missing_count > 0:\n",
    "        print(f\"âš ï¸ ê²°ì¸¡ê°’: {missing_count}ê°œ\")\n",
    "        missing_cols = {k: v for k, v in info['missing_values'].items() if v > 0}\n",
    "        for col, count in missing_cols.items():\n",
    "            print(f\"    - {col}: {count}ê°œ ({count/info['shape'][0]*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"âœ… ê²°ì¸¡ê°’: ì—†ìŒ\")\n",
    "    \n",
    "    # íƒ€ê²Ÿ ë¶„í¬ (ìˆëŠ” ê²½ìš°)\n",
    "    if 'target_distribution' in info:\n",
    "        print(f\"ğŸ¯ íƒ€ê²Ÿ ë¶„í¬: {info['target_distribution']}\")\n",
    "        target_ratio = info['target_distribution'].get(1, 0) / info['shape'][0]\n",
    "        print(f\"  - ê¸ì • í´ë˜ìŠ¤ ë¹„ìœ¨: {target_ratio:.2%}\")\n",
    "    \n",
    "    # ì»¬ëŸ¼ ë¯¸ë¦¬ë³´ê¸°\n",
    "    print(f\"\\nğŸ“‹ ì»¬ëŸ¼ ëª©ë¡ (ì²˜ìŒ 10ê°œ):\")\n",
    "    for i, col in enumerate(info['columns'][:10]):\n",
    "        dtype = info['dtypes'][col]\n",
    "        print(f\"  {i+1:2d}. {col:<20} ({dtype})\")\n",
    "    \n",
    "    if len(info['columns']) > 10:\n",
    "        print(f\"     ... ì™¸ {len(info['columns'])-10}ê°œ ì»¬ëŸ¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Kaggle ë°ì´í„° ì‹¬í™” ë¶„ì„\n",
    "\n",
    "Kaggle Safe Driving ë°ì´í„°ì˜ íŠ¹ì„±ì„ ìì„¸íˆ ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle í›ˆë ¨ ë°ì´í„° ì„ íƒ\n",
    "if 'kaggle_train' in all_data:\n",
    "    kaggle_data = all_data['kaggle_train']\n",
    "    print(f\"ğŸ“Š Kaggle í›ˆë ¨ ë°ì´í„° ë¶„ì„: {kaggle_data.shape}\")\n",
    "    print(f\"ğŸ“‹ ì²˜ìŒ 5í–‰ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    display(kaggle_data.head())\n",
    "else:\n",
    "    print(\"âš ï¸ Kaggle í›ˆë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kaggle_train' in all_data and 'target' in kaggle_data.columns:\n",
    "    # íƒ€ê²Ÿ ë¶„í¬ ì‹œê°í™”\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # ë§‰ëŒ€ ê·¸ë˜í”„\n",
    "    target_counts = kaggle_data['target'].value_counts()\n",
    "    axes[0].bar(target_counts.index, target_counts.values, color=['skyblue', 'salmon'])\n",
    "    axes[0].set_title('íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('íƒ€ê²Ÿ ê°’')\n",
    "    axes[0].set_ylabel('ë¹ˆë„')\n",
    "    \n",
    "    # ë¹„ìœ¨ í‘œì‹œ\n",
    "    for i, v in enumerate(target_counts.values):\n",
    "        axes[0].text(i, v + target_counts.max() * 0.01, \n",
    "                    f'{v:,}\\n({v/len(kaggle_data)*100:.1f}%)', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # íŒŒì´ ì°¨íŠ¸\n",
    "    labels = ['ì•ˆì „ (0)', 'ìœ„í—˜ (1)']\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    axes[1].pie(target_counts.values, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "               startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "    axes[1].set_title('íƒ€ê²Ÿ ë³€ìˆ˜ ë¹„ìœ¨', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„\n",
    "    imbalance_ratio = target_counts[0] / target_counts[1]\n",
    "    print(f\"\\nğŸ“ˆ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„:\")\n",
    "    print(f\"  - ì•ˆì „(0): {target_counts[0]:,}ê°œ ({target_counts[0]/len(kaggle_data)*100:.1f}%)\")\n",
    "    print(f\"  - ìœ„í—˜(1): {target_counts[1]:,}ê°œ ({target_counts[1]/len(kaggle_data)*100:.1f}%)\")\n",
    "    print(f\"  - ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.1f}:1\")\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"âš ï¸ ì‹¬í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. SMOTEë‚˜ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "    elif imbalance_ratio > 5:\n",
    "        print(f\"âš ï¸ ì¤‘ê°„ ì •ë„ì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ìˆìŠµë‹ˆë‹¤. ê· í˜• ì¡°ì • ê¸°ë²•ì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(f\"âœ… í´ë˜ìŠ¤ ê· í˜•ì´ ë¹„êµì  ì–‘í˜¸í•©ë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âš ï¸ íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¢ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kaggle_train' in all_data:\n",
    "    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì„ ë³„\n",
    "    numeric_cols = kaggle_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'id' in numeric_cols:\n",
    "        numeric_cols.remove('id')\n",
    "    if 'target' in numeric_cols:\n",
    "        numeric_cols.remove('target')\n",
    "    \n",
    "    print(f\"ğŸ“Š ìˆ˜ì¹˜í˜• ë³€ìˆ˜ {len(numeric_cols)}ê°œ ë¶„ì„\")\n",
    "    \n",
    "    # ê¸°ë³¸ í†µê³„ëŸ‰\n",
    "    print(f\"\\nğŸ“ˆ ê¸°ë³¸ í†µê³„ëŸ‰:\")\n",
    "    display(kaggle_data[numeric_cols].describe().round(3))\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ ë¶„ì„ (ìƒ˜í”Œë§ìœ¼ë¡œ ì†ë„ í–¥ìƒ)\n",
    "    sample_size = min(len(kaggle_data), 5000)\n",
    "    sample_data = kaggle_data.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ë“¤ ì°¾ê¸°\n",
    "    if 'target' in kaggle_data.columns:\n",
    "        correlations = sample_data[numeric_cols + ['target']].corr()['target'].drop('target')\n",
    "        correlations_abs = correlations.abs().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nğŸ¯ íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ TOP 10:\")\n",
    "        for i, (var, corr) in enumerate(correlations_abs.head(10).items()):\n",
    "            print(f\"  {i+1:2d}. {var:<20} : {correlations[var]:>7.4f} (|{corr:.4f}|)\")\n",
    "        \n",
    "        # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ (ìƒìœ„ ë³€ìˆ˜ë“¤ë§Œ)\n",
    "        top_vars = correlations_abs.head(15).index.tolist() + ['target']\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        corr_matrix = sample_data[top_vars].corr()\n",
    "        \n",
    "        # ë§ˆìŠ¤í¬ ìƒì„± (ìƒì‚¼ê°í˜• ìˆ¨ê¹€)\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        \n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
    "                   square=True, fmt='.3f', cbar_kws={\"shrink\": .8})\n",
    "        plt.title('ì£¼ìš” ë³€ìˆ˜ë“¤ì˜ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Kaggle ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š ì£¼ìš” ë³€ìˆ˜ ë¶„í¬ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kaggle_train' in all_data and len(numeric_cols) > 0:\n",
    "    # ìƒìœ„ ì¤‘ìš” ë³€ìˆ˜ë“¤ì˜ ë¶„í¬ ì‹œê°í™”\n",
    "    if 'target' in kaggle_data.columns:\n",
    "        top_vars = correlations_abs.head(8).index.tolist()\n",
    "    else:\n",
    "        top_vars = numeric_cols[:8]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, var in enumerate(top_vars):\n",
    "        # íˆìŠ¤í† ê·¸ë¨\n",
    "        axes[i].hist(kaggle_data[var].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_title(f'{var}', fontweight='bold')\n",
    "        axes[i].set_ylabel('ë¹ˆë„')\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„ëŸ‰ í‘œì‹œ\n",
    "        mean_val = kaggle_data[var].mean()\n",
    "        median_val = kaggle_data[var].median()\n",
    "        axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.8, label=f'í‰ê· : {mean_val:.3f}')\n",
    "        axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.8, label=f'ì¤‘ì•™ê°’: {median_val:.3f}')\n",
    "        axes[i].legend(fontsize=8)\n",
    "        \n",
    "        # ì´ìƒê°’ í‘œì‹œ\n",
    "        Q1 = kaggle_data[var].quantile(0.25)\n",
    "        Q3 = kaggle_data[var].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_threshold = Q3 + 1.5 * IQR\n",
    "        outliers = kaggle_data[var] > outlier_threshold\n",
    "        \n",
    "        if outliers.sum() > 0:\n",
    "            axes[i].axvline(outlier_threshold, color='orange', linestyle=':', alpha=0.8)\n",
    "            axes[i].text(0.95, 0.95, f'ì´ìƒê°’: {outliers.sum()}ê°œ', \n",
    "                        transform=axes[i].transAxes, ha='right', va='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle('ì£¼ìš” ë³€ìˆ˜ë“¤ì˜ ë¶„í¬', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ë¶„í¬ íŠ¹ì„± ë¶„ì„\n",
    "    print(f\"\\nğŸ“Š ë¶„í¬ íŠ¹ì„± ë¶„ì„:\")\n",
    "    for var in top_vars:\n",
    "        data = kaggle_data[var].dropna()\n",
    "        \n",
    "        # ì™œë„ì™€ ì²¨ë„\n",
    "        skewness = stats.skew(data)\n",
    "        kurtosis = stats.kurtosis(data)\n",
    "        \n",
    "        # ì •ê·œì„± ê²€ì • (ìƒ˜í”Œë§)\n",
    "        if len(data) > 5000:\n",
    "            test_sample = data.sample(5000, random_state=42)\n",
    "        else:\n",
    "            test_sample = data\n",
    "        \n",
    "        _, p_value = stats.normaltest(test_sample)\n",
    "        \n",
    "        print(f\"  ğŸ“ˆ {var:<20}: ì™œë„={skewness:>6.2f}, ì²¨ë„={kurtosis:>6.2f}, p-value={p_value:.2e}\")\n",
    "        \n",
    "        if abs(skewness) > 2:\n",
    "            print(f\"      âš ï¸ ì‹¬í•œ ë¹„ëŒ€ì¹­ ë¶„í¬ (ë¡œê·¸ ë³€í™˜ ê³ ë ¤)\")\n",
    "        elif abs(skewness) > 1:\n",
    "            print(f\"      âš ï¸ ì¤‘ê°„ ì •ë„ ë¹„ëŒ€ì¹­ ë¶„í¬\")\n",
    "        \n",
    "        if p_value < 0.001:\n",
    "            print(f\"      âš ï¸ ë¹„ì •ê·œë¶„í¬ (ë³€í™˜ í•„ìš”)\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ ë¶„ì„í•  ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš— ì„¼ì„œ ë°ì´í„° ë¶„ì„\n",
    "\n",
    "GPS, ê°€ì†ë„ê³„, ìì´ë¡œìŠ¤ì½”í”„ ë“± ì„¼ì„œ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¼ì„œ ë°ì´í„° ì„ ë³„\n",
    "sensor_datasets = {k: v for k, v in all_data.items() if 'sensor' in k}\n",
    "\n",
    "print(f\"ğŸ” ì„¼ì„œ ë°ì´í„°ì…‹ {len(sensor_datasets)}ê°œ ë°œê²¬:\")\n",
    "for name, df in sensor_datasets.items():\n",
    "    print(f\"  ğŸ“¡ {name}: {df.shape}\")\n",
    "\n",
    "# ì„¼ì„œ ë°ì´í„° í†µí•© ë¶„ì„\n",
    "if sensor_datasets:\n",
    "    # ì²« ë²ˆì§¸ ì„¼ì„œ ë°ì´í„°ì…‹ìœ¼ë¡œ ìƒì„¸ ë¶„ì„\n",
    "    first_sensor = list(sensor_datasets.keys())[0]\n",
    "    sensor_data = sensor_datasets[first_sensor]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {first_sensor} ë°ì´í„° ìƒì„¸ ë¶„ì„:\")\n",
    "    print(f\"ğŸ“‹ ì»¬ëŸ¼ ì •ë³´:\")\n",
    "    display(sensor_data.info())\n",
    "    print(f\"\\nğŸ“‹ ì²˜ìŒ 5í–‰:\")\n",
    "    display(sensor_data.head())\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ ì„¼ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ—ºï¸ GPS ë°ì´í„° ë¶„ì„ (ìˆëŠ” ê²½ìš°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPS ë°ì´í„° ì°¾ê¸°\n",
    "gps_data = None\n",
    "for name, df in sensor_datasets.items():\n",
    "    if 'gps' in name and all(col in df.columns for col in ['latitude', 'longitude']):\n",
    "        gps_data = df\n",
    "        print(f\"ğŸ—ºï¸ GPS ë°ì´í„° ë°œê²¬: {name}\")\n",
    "        break\n",
    "\n",
    "if gps_data is not None:\n",
    "    # GPS ë°ì´í„° ê¸°ë³¸ í†µê³„\n",
    "    print(f\"\\nğŸ“Š GPS ë°ì´í„° ê¸°ë³¸ í†µê³„:\")\n",
    "    gps_stats = gps_data[['latitude', 'longitude']].describe()\n",
    "    display(gps_stats)\n",
    "    \n",
    "    # ì´ë™ ë²”ìœ„ ê³„ì‚°\n",
    "    lat_range = gps_data['latitude'].max() - gps_data['latitude'].min()\n",
    "    lon_range = gps_data['longitude'].max() - gps_data['longitude'].min()\n",
    "    \n",
    "    print(f\"\\nğŸ—ºï¸ ì´ë™ ë²”ìœ„:\")\n",
    "    print(f\"  - ìœ„ë„ ë²”ìœ„: {lat_range:.6f}Â° ({lat_range * 111:.1f} km)\")\n",
    "    print(f\"  - ê²½ë„ ë²”ìœ„: {lon_range:.6f}Â° ({lon_range * 111:.1f} km)\")\n",
    "    \n",
    "    # GPS ê¶¤ì  ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # ì „ì²´ ê¶¤ì \n",
    "    plt.scatter(gps_data['longitude'], gps_data['latitude'], \n",
    "               c=range(len(gps_data)), cmap='viridis', alpha=0.6, s=1)\n",
    "    plt.colorbar(label='ì‹œê°„ ìˆœì„œ')\n",
    "    plt.xlabel('ê²½ë„ (Longitude)')\n",
    "    plt.ylabel('ìœ„ë„ (Latitude)')\n",
    "    plt.title('GPS ì´ë™ ê¶¤ì ', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # ì‹œì‘ì ê³¼ ëì  í‘œì‹œ\n",
    "    plt.scatter(gps_data['longitude'].iloc[0], gps_data['latitude'].iloc[0], \n",
    "               color='green', s=100, marker='o', label='ì‹œì‘ì ', zorder=5)\n",
    "    plt.scatter(gps_data['longitude'].iloc[-1], gps_data['latitude'].iloc[-1], \n",
    "               color='red', s=100, marker='s', label='ëì ', zorder=5)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ì†ë„ ë¶„ì„ (ìˆëŠ” ê²½ìš°)\n",
    "    if 'speed_kmh' in gps_data.columns:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # ì‹œê°„ë³„ ì†ë„ ë³€í™”\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(gps_data.index, gps_data['speed_kmh'], linewidth=0.8, alpha=0.7)\n",
    "        plt.title('ì‹œê°„ë³„ ì†ë„ ë³€í™”', fontweight='bold')\n",
    "        plt.xlabel('ì‹œê°„ ì¸ë±ìŠ¤')\n",
    "        plt.ylabel('ì†ë„ (km/h)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ì†ë„ ë¶„í¬\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(gps_data['speed_kmh'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(gps_data['speed_kmh'].mean(), color='red', linestyle='--', \n",
    "                   label=f'í‰ê· : {gps_data[\"speed_kmh\"].mean():.1f} km/h')\n",
    "        plt.title('ì†ë„ ë¶„í¬', fontweight='bold')\n",
    "        plt.xlabel('ì†ë„ (km/h)')\n",
    "        plt.ylabel('ë¹ˆë„')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ì†ë„ í†µê³„\n",
    "        print(f\"\\nğŸš— ì†ë„ í†µê³„:\")\n",
    "        print(f\"  - í‰ê·  ì†ë„: {gps_data['speed_kmh'].mean():.1f} km/h\")\n",
    "        print(f\"  - ìµœëŒ€ ì†ë„: {gps_data['speed_kmh'].max():.1f} km/h\")\n",
    "        print(f\"  - ì†ë„ í‘œì¤€í¸ì°¨: {gps_data['speed_kmh'].std():.1f} km/h\")\n",
    "        \n",
    "        # ìœ„í—˜ ì†ë„ êµ¬ê°„ ë¶„ì„\n",
    "        high_speed = (gps_data['speed_kmh'] > 80).sum()\n",
    "        very_high_speed = (gps_data['speed_kmh'] > 120).sum()\n",
    "        \n",
    "        print(f\"  - ê³ ì† êµ¬ê°„ (>80km/h): {high_speed}íšŒ ({high_speed/len(gps_data)*100:.1f}%)\")\n",
    "        print(f\"  - ì´ˆê³ ì† êµ¬ê°„ (>120km/h): {very_high_speed}íšŒ ({very_high_speed/len(gps_data)*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ GPS ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“± ê°€ì†ë„ê³„ ë°ì´í„° ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°€ì†ë„ê³„ ë°ì´í„° ì°¾ê¸°\n",
    "acc_data = None\n",
    "for name, df in sensor_datasets.items():\n",
    "    if 'accelerometer' in name or any('acc_' in col for col in df.columns):\n",
    "        acc_data = df\n",
    "        print(f\"ğŸ“± ê°€ì†ë„ê³„ ë°ì´í„° ë°œê²¬: {name}\")\n",
    "        break\n",
    "\n",
    "if acc_data is not None:\n",
    "    # ê°€ì†ë„ ì»¬ëŸ¼ ì‹ë³„\n",
    "    acc_cols = [col for col in acc_data.columns if col.startswith('acc_')]\n",
    "    \n",
    "    if len(acc_cols) >= 3:\n",
    "        print(f\"\\nğŸ“Š ê°€ì†ë„ê³„ ë°ì´í„° ë¶„ì„ ({len(acc_cols)}ê°œ ì¶•):\")\n",
    "        \n",
    "        # ê°€ì†ë„ ê¸°ë³¸ í†µê³„\n",
    "        acc_stats = acc_data[acc_cols].describe()\n",
    "        display(acc_stats)\n",
    "        \n",
    "        # 3ì¶• ê°€ì†ë„ ì‹œê°í™”\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # ì‹œê³„ì—´ í”Œë¡¯\n",
    "        sample_size = min(1000, len(acc_data))\n",
    "        sample_indices = np.linspace(0, len(acc_data)-1, sample_size, dtype=int)\n",
    "        \n",
    "        axes[0, 0].plot(sample_indices, acc_data[acc_cols[0]].iloc[sample_indices], label=acc_cols[0], alpha=0.7)\n",
    "        axes[0, 0].plot(sample_indices, acc_data[acc_cols[1]].iloc[sample_indices], label=acc_cols[1], alpha=0.7)\n",
    "        axes[0, 0].plot(sample_indices, acc_data[acc_cols[2]].iloc[sample_indices], label=acc_cols[2], alpha=0.7)\n",
    "        axes[0, 0].set_title('3ì¶• ê°€ì†ë„ ì‹œê³„ì—´', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('ì‹œê°„ ì¸ë±ìŠ¤')\n",
    "        axes[0, 0].set_ylabel('ê°€ì†ë„ (m/sÂ²)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ì´ ê°€ì†ë„ í¬ê¸° ê³„ì‚°\n",
    "        total_acc = np.sqrt(acc_data[acc_cols[0]]**2 + acc_data[acc_cols[1]]**2 + acc_data[acc_cols[2]]**2)\n",
    "        \n",
    "        # ì´ ê°€ì†ë„ ë¶„í¬\n",
    "        axes[0, 1].hist(total_acc.dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].axvline(total_acc.mean(), color='red', linestyle='--', \n",
    "                          label=f'í‰ê· : {total_acc.mean():.2f}')\n",
    "        axes[0, 1].set_title('ì´ ê°€ì†ë„ í¬ê¸° ë¶„í¬', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('ì´ ê°€ì†ë„ (m/sÂ²)')\n",
    "        axes[0, 1].set_ylabel('ë¹ˆë„')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ê°€ì†ë„ ë³€ë™ì„± (Jerk) ë¶„ì„\n",
    "        jerk = total_acc.diff().abs()\n",
    "        axes[1, 0].plot(sample_indices[1:], jerk.iloc[sample_indices[1:]], alpha=0.7)\n",
    "        axes[1, 0].set_title('ê°€ì†ë„ ë³€ë™ì„± (Jerk)', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('ì‹œê°„ ì¸ë±ìŠ¤')\n",
    "        axes[1, 0].set_ylabel('Jerk (m/sÂ³)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ê³ ê°€ì†ë„ ì´ë²¤íŠ¸ íƒì§€\n",
    "        high_acc_threshold = total_acc.quantile(0.95)\n",
    "        high_acc_events = total_acc > high_acc_threshold\n",
    "        \n",
    "        axes[1, 1].scatter(range(len(total_acc)), total_acc, alpha=0.3, s=1)\n",
    "        axes[1, 1].scatter(range(len(total_acc))[high_acc_events], \n",
    "                          total_acc[high_acc_events], color='red', s=2, \n",
    "                          label=f'ê³ ê°€ì†ë„ ì´ë²¤íŠ¸ ({high_acc_events.sum()}ê°œ)')\n",
    "        axes[1, 1].axhline(high_acc_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[1, 1].set_title('ê³ ê°€ì†ë„ ì´ë²¤íŠ¸ íƒì§€', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('ì‹œê°„ ì¸ë±ìŠ¤')\n",
    "        axes[1, 1].set_ylabel('ì´ ê°€ì†ë„ (m/sÂ²)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ê°€ì†ë„ í†µê³„ ìš”ì•½\n",
    "        print(f\"\\nğŸ“Š ê°€ì†ë„ ë¶„ì„ ê²°ê³¼:\")\n",
    "        print(f\"  - í‰ê·  ì´ ê°€ì†ë„: {total_acc.mean():.2f} Â± {total_acc.std():.2f} m/sÂ²\")\n",
    "        print(f\"  - ìµœëŒ€ ì´ ê°€ì†ë„: {total_acc.max():.2f} m/sÂ²\")\n",
    "        print(f\"  - ê³ ê°€ì†ë„ ì´ë²¤íŠ¸ (ìƒìœ„ 5%): {high_acc_events.sum()}ê°œ ({high_acc_events.sum()/len(total_acc)*100:.1f}%)\")\n",
    "        print(f\"  - í‰ê·  Jerk: {jerk.mean():.2f} Â± {jerk.std():.2f} m/sÂ³\")\n",
    "        \n",
    "        # ì¤‘ë ¥ ì œê±° ë¶„ì„ (Zì¶•ì´ ì¤‘ë ¥ ë°©í–¥ì´ë¼ê³  ê°€ì •)\n",
    "        if 'acc_z' in acc_cols:\n",
    "            gravity_removed = np.sqrt(acc_data['acc_x']**2 + acc_data['acc_y']**2 + (acc_data['acc_z'] - 9.8)**2)\n",
    "            print(f\"  - ì¤‘ë ¥ ì œê±° í›„ í‰ê·  ê°€ì†ë„: {gravity_removed.mean():.2f} Â± {gravity_removed.std():.2f} m/sÂ²\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"âš ï¸ 3ì¶• ê°€ì†ë„ ë°ì´í„°ê°€ ì™„ì „í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°œê²¬ëœ ì»¬ëŸ¼: {acc_cols}\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ ê°€ì†ë„ê³„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¯¸ë¦¬ë³´ê¸°\n",
    "\n",
    "ì „ì²˜ë¦¬ ëª¨ë“ˆì„ ì‚¬ìš©í•´ì„œ íŒŒìƒ ë³€ìˆ˜ë“¤ì„ ìƒì„±í•˜ê³  ê·¸ íš¨ê³¼ë¥¼ ì‚´í´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ ëª¨ë“ˆ ì´ˆê¸°í™”\n",
    "preprocessor = SafeDrivingPreprocessor()\n",
    "\n",
    "# ì„¼ì„œ ë°ì´í„°ë¡œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í…ŒìŠ¤íŠ¸\n",
    "if sensor_datasets:\n",
    "    # ì²« ë²ˆì§¸ ì„¼ì„œ ë°ì´í„° ì„ íƒ\n",
    "    test_data = list(sensor_datasets.values())[0].copy()\n",
    "    \n",
    "    print(f\"ğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"ì›ë³¸ ë°ì´í„°: {test_data.shape}\")\n",
    "    \n",
    "    # íŒŒìƒ ë³€ìˆ˜ ìƒì„±\n",
    "    enhanced_data = preprocessor.create_derived_features(test_data)\n",
    "    print(f\"íŒŒìƒ ë³€ìˆ˜ ì¶”ê°€ í›„: {enhanced_data.shape}\")\n",
    "    \n",
    "    # ìƒˆë¡œ ìƒì„±ëœ ë³€ìˆ˜ë“¤\n",
    "    new_features = [col for col in enhanced_data.columns if col not in test_data.columns]\n",
    "    print(f\"\\nğŸ†• ìƒˆë¡œ ìƒì„±ëœ í”¼ì²˜ {len(new_features)}ê°œ:\")\n",
    "    \n",
    "    for i, feature in enumerate(new_features[:15]):  # ìƒìœ„ 15ê°œë§Œ í‘œì‹œ\n",
    "        print(f\"  {i+1:2d}. {feature}\")\n",
    "    \n",
    "    if len(new_features) > 15:\n",
    "        print(f\"     ... ì™¸ {len(new_features)-15}ê°œ\")\n",
    "    \n",
    "    # ì¤‘ìš”í•œ ì•ˆì „ ìš´ì „ í”¼ì²˜ë“¤ ì‹œê°í™”\n",
    "    safety_features = [col for col in new_features \n",
    "                      if any(keyword in col.lower() for keyword in \n",
    "                            ['harsh', 'speed_change', 'acceleration', 'turn'])]\n",
    "    \n",
    "    if len(safety_features) > 0:\n",
    "        n_features = min(4, len(safety_features))\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            feature = safety_features[i]\n",
    "            data = enhanced_data[feature].dropna()\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                axes[i].hist(data, bins=30, alpha=0.7, edgecolor='black')\n",
    "                axes[i].set_title(feature, fontweight='bold')\n",
    "                axes[i].set_ylabel('ë¹ˆë„')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # ê¸°ë³¸ í†µê³„ëŸ‰ í‘œì‹œ\n",
    "                mean_val = data.mean()\n",
    "                axes[i].axvline(mean_val, color='red', linestyle='--', \n",
    "                               label=f'í‰ê· : {mean_val:.3f}')\n",
    "                axes[i].legend()\n",
    "        \n",
    "        plt.suptitle('ì£¼ìš” ì•ˆì „ ìš´ì „ íŒŒìƒ í”¼ì²˜ë“¤', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ì•ˆì „ ìš´ì „ ì ìˆ˜ í”¼ì²˜ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "        scored_data = preprocessor.create_driving_score_features(enhanced_data)\n",
    "        \n",
    "        score_features = [col for col in scored_data.columns \n",
    "                         if 'score' in col.lower() or 'freq' in col.lower()]\n",
    "        \n",
    "        if score_features:\n",
    "            print(f\"\\nğŸ† ì•ˆì „ ìš´ì „ ì ìˆ˜ í”¼ì²˜ë“¤:\")\n",
    "            for feature in score_features:\n",
    "                data = scored_data[feature].dropna()\n",
    "                if len(data) > 0:\n",
    "                    print(f\"  - {feature:<25}: í‰ê·  {data.mean():.3f} (ë²”ìœ„: {data.min():.3f} ~ {data.max():.3f})\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì„¼ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ ì¢…í•© í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_datasets = len(all_data)\n",
    "total_samples = sum(df.shape[0] for df in all_data.values())\n",
    "total_features = sum(df.shape[1] for df in all_data.values())\n",
    "\n",
    "print(f\"\\nğŸ“‹ ì „ì²´ ë°ì´í„° ìš”ì•½:\")\n",
    "print(f\"  - ë°ì´í„°ì…‹ ìˆ˜: {total_datasets}ê°œ\")\n",
    "print(f\"  - ì´ ìƒ˜í”Œ ìˆ˜: {total_samples:,}ê°œ\")\n",
    "print(f\"  - ì´ í”¼ì²˜ ìˆ˜: {total_features:,}ê°œ\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ë³„ í’ˆì§ˆ í‰ê°€\n",
    "quality_scores = []\n",
    "\n",
    "for name, df in all_data.items():\n",
    "    # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°\n",
    "    missing_ratio = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])\n",
    "    duplicate_ratio = df.duplicated().sum() / df.shape[0]\n",
    "    \n",
    "    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ë¶„ì‚°\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        variance_score = df[numeric_cols].var().mean()\n",
    "    else:\n",
    "        variance_score = 0\n",
    "    \n",
    "    # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ (0-100)\n",
    "    quality_score = (\n",
    "        (1 - missing_ratio) * 40 +  # ê²°ì¸¡ê°’ì´ ì ì„ìˆ˜ë¡ ì¢‹ìŒ\n",
    "        (1 - duplicate_ratio) * 30 +  # ì¤‘ë³µì´ ì ì„ìˆ˜ë¡ ì¢‹ìŒ\n",
    "        min(variance_score / 10, 1) * 30  # ì ë‹¹í•œ ë¶„ì‚°ì´ ì¢‹ìŒ\n",
    "    ) * 100\n",
    "    \n",
    "    quality_scores.append({\n",
    "        'dataset': name,\n",
    "        'samples': df.shape[0],\n",
    "        'features': df.shape[1],\n",
    "        'missing_ratio': missing_ratio,\n",
    "        'duplicate_ratio': duplicate_ratio,\n",
    "        'quality_score': quality_score\n",
    "    })\n",
    "\n",
    "# í’ˆì§ˆ ì ìˆ˜ ë°ì´í„°í”„ë ˆì„\n",
    "quality_df = pd.DataFrame(quality_scores)\n",
    "quality_df = quality_df.sort_values('quality_score', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ† ë°ì´í„°ì…‹ë³„ í’ˆì§ˆ ì ìˆ˜ (100ì  ë§Œì ):\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in quality_df.iterrows():\n",
    "    score_emoji = \"ğŸŸ¢\" if row['quality_score'] >= 80 else \"ğŸŸ¡\" if row['quality_score'] >= 60 else \"ğŸ”´\"\n",
    "    print(f\"{score_emoji} {row['dataset']:<20}: {row['quality_score']:>5.1f}ì  \"\n",
    "          f\"(ê²°ì¸¡ë¥ : {row['missing_ratio']*100:>4.1f}%, ì¤‘ë³µë¥ : {row['duplicate_ratio']*100:>4.1f}%)\")\n",
    "\n",
    "# í’ˆì§ˆ ê°œì„  ì œì•ˆ\n",
    "print(f\"\\nğŸ’¡ í’ˆì§ˆ ê°œì„  ì œì•ˆ:\")\n",
    "\n",
    "low_quality = quality_df[quality_df['quality_score'] < 70]\n",
    "if len(low_quality) > 0:\n",
    "    print(f\"  âš ï¸ í’ˆì§ˆì´ ë‚®ì€ ë°ì´í„°ì…‹ {len(low_quality)}ê°œ:\")\n",
    "    for _, row in low_quality.iterrows():\n",
    "        print(f\"    - {row['dataset']}: \", end=\"\")\n",
    "        if row['missing_ratio'] > 0.1:\n",
    "            print(\"ê²°ì¸¡ê°’ ì²˜ë¦¬ í•„ìš” \", end=\"\")\n",
    "        if row['duplicate_ratio'] > 0.05:\n",
    "            print(\"ì¤‘ë³µê°’ ì œê±° í•„ìš” \", end=\"\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"  âœ… ëª¨ë“  ë°ì´í„°ì…‹ì´ ì–‘í˜¸í•œ í’ˆì§ˆì„ ë³´ì…ë‹ˆë‹¤!\")\n",
    "\n",
    "# ì „ì²´ í’ˆì§ˆ ì ìˆ˜\n",
    "overall_quality = quality_df['quality_score'].mean()\n",
    "overall_emoji = \"ğŸŸ¢\" if overall_quality >= 80 else \"ğŸŸ¡\" if overall_quality >= 60 else \"ğŸ”´\"\n",
    "\n",
    "print(f\"\\n{overall_emoji} ì „ì²´ ë°ì´í„° í’ˆì§ˆ ì ìˆ˜: {overall_quality:.1f}ì \")\n",
    "\n",
    "if overall_quality >= 80:\n",
    "    print(f\"  âœ… ìš°ìˆ˜í•œ ë°ì´í„° í’ˆì§ˆ! ëª¨ë¸ í•™ìŠµì— ì í•©í•©ë‹ˆë‹¤.\")\n",
    "elif overall_quality >= 60:\n",
    "    print(f\"  âš ï¸ ë³´í†µ ìˆ˜ì¤€ì˜ ë°ì´í„° í’ˆì§ˆ. ì¼ë¶€ ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"  ğŸ”´ ë°ì´í„° í’ˆì§ˆ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤. ìƒë‹¹í•œ ì „ì²˜ë¦¬ ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì£¼ìš” ë°œê²¬ì‚¬í•­ ë° ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ğŸ“Š ì£¼ìš” ë°œê²¬ì‚¬í•­\n",
    "\n",
    "ì´ EDAë¥¼ í†µí•´ ë°œê²¬ëœ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **ë°ì´í„° êµ¬ì¡° ë¶„ì„**\n",
    "   - ì „ì²´ ë°ì´í„°ì…‹ êµ¬ì„±ê³¼ ê°ê°ì˜ íŠ¹ì„± íŒŒì•… ì™„ë£Œ\n",
    "   - ê²°ì¸¡ê°’ê³¼ ì´ìƒê°’ íŒ¨í„´ ì‹ë³„\n",
    "\n",
    "2. **ì•ˆì „ ìš´ì „ íŒ¨í„´**\n",
    "   - íƒ€ê²Ÿ ë³€ìˆ˜ì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì •ë„ í™•ì¸\n",
    "   - ì£¼ìš” ìœ„í—˜ ìš”ì¸ë“¤ê³¼ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "\n",
    "3. **ì„¼ì„œ ë°ì´í„° íŠ¹ì„±**\n",
    "   - GPS ê¶¤ì ê³¼ ì†ë„ íŒ¨í„´ ë¶„ì„\n",
    "   - ê°€ì†ë„ê³„ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆì™€ ì‹ í˜¸ íŠ¹ì„±\n",
    "\n",
    "4. **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê°€ëŠ¥ì„±**\n",
    "   - ê¸‰ì •ì§€/ê¸‰ê°€ì† ê°ì§€ ê°€ëŠ¥ì„± í™•ì¸\n",
    "   - ìš´ì „ íŒ¨í„´ ê¸°ë°˜ ì•ˆì „ ì ìˆ˜ ê³„ì‚° ê°€ëŠ¥ì„±\n",
    "\n",
    "### ğŸ”„ ë‹¤ìŒ ë‹¨ê³„ (Phase 3)\n",
    "\n",
    "1. **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¬í™”**\n",
    "   - ìš´ì „ íŒ¨í„´ ê¸°ë°˜ í”¼ì²˜ ê°œë°œ\n",
    "   - ì‹œê°„ ìœˆë„ìš° ê¸°ë°˜ ì§‘ê³„ í”¼ì²˜ ìƒì„±\n",
    "\n",
    "2. **ëª¨ë¸ ê°œë°œ ì¤€ë¹„**\n",
    "   - í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²° ì „ëµ ìˆ˜ë¦½\n",
    "   - í”¼ì²˜ ì„ íƒ ë° ì°¨ì› ì¶•ì†Œ ê²€í† \n",
    "\n",
    "3. **ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬ì¶•**\n",
    "   - XGBoost/LightGBM ëª¨ë¸ ê°œë°œ\n",
    "   - PyTorch MLP ëª¨ë¸ êµ¬ì¶•\n",
    "\n",
    "---\n",
    "\n",
    "**ë‹¤ìŒ ë…¸íŠ¸ë¶**: `02_Feature_Engineering.ipynb` ì—ì„œ ë” ìƒì„¸í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰ EDA ì™„ë£Œ!\")\n",
    "print(\"ë‹¤ìŒ ë‹¨ê³„: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ëª¨ë¸ ê°œë°œ\")\n",
    "\n",
    "# ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì €ì¥ (ì„ íƒì‚¬í•­)\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "eda_summary = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'total_datasets': len(all_data),\n",
    "    'total_samples': sum(df.shape[0] for df in all_data.values()),\n",
    "    'total_features': sum(df.shape[1] for df in all_data.values()),\n",
    "    'data_quality': {\n",
    "        'overall_score': overall_quality,\n",
    "        'datasets': quality_scores\n",
    "    },\n",
    "    'next_steps': [\n",
    "        'Feature Engineering',\n",
    "        'Model Development',\n",
    "        'Hyperparameter Optimization'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "with open('../results/eda_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(eda_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"ğŸ“„ EDA ìš”ì•½ ê²°ê³¼ê°€ '../results/eda_summary.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}